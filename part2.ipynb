{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d643bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_community.vectorstores import Chroma\n",
    "load_dotenv()\n",
    "\n",
    "# Set Groq variables using OpenAI-compatible names\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"USER_AGENT\"] = \"MyAppName/1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85b58142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "file_pointer = open(\"reference.txt\", \"w\")\n",
    "for doc in blog_docs:\n",
    "    file_pointer.write(doc.page_content+\"\\n\")\n",
    "file_pointer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e0ff1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "\n",
    "with open(\"reference.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "# 2. Wrap in a Document object\n",
    "document = Document(page_content=file_content)\n",
    "docs = [document]\n",
    "\n",
    "# 3. Split the document\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 4. Embed and store in Chroma\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")\n",
    ")\n",
    "\n",
    "# 5. Create a retriever (top 1 result)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d03a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can large language models break down complex problems into smaller steps?\n",
      "What are the methods used to decompose a task when working with large language models?\n",
      "Explain the concept of task decomposition in the context of large language models.\n",
      "What are the benefits of using task decomposition with large language models?\n",
      "What are some examples of task decomposition strategies employed by large language models?\n",
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructe\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm_model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "chain = prompt | llm_model |StrOutputParser()\n",
    "respone = chain.invoke({\"question\": \"What is task decomposition for llm models\"})\n",
    "print(respone)\n",
    "questions = respone.split(\"\\n\")\n",
    "\n",
    "# Store all retrieved docs\n",
    "all_results = []\n",
    "\n",
    "for q in questions:\n",
    "    if q.strip():  # skip any empty lines\n",
    "        docs = retriever.get_relevant_documents(q)\n",
    "        all_results.extend(docs)\n",
    "\n",
    "# Optional: remove duplicates (based on page_content)\n",
    "unique_results = {doc.page_content: doc for doc in all_results}.values()\n",
    "\n",
    "# Print or use results\n",
    "for doc in unique_results:\n",
    "    print(doc.page_content[:300])  # print first 300 chars for readability\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5812616d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f1d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2a585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73450d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79badfa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
